{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be236001-5f80-4a80-9d2d-53eeaaaa42d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 19 03:22:34 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A10G                    On  |   00000000:00:1B.0 Off |                    0 |\n",
      "|  0%   22C    P8             22W /  300W |       1MiB /  23028MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A10G                    On  |   00000000:00:1C.0 Off |                    0 |\n",
      "|  0%   21C    P8             22W /  300W |       1MiB /  23028MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA A10G                    On  |   00000000:00:1D.0 Off |                    0 |\n",
      "|  0%   22C    P8             22W /  300W |       1MiB /  23028MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA A10G                    On  |   00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   21C    P8             22W /  300W |       1MiB /  23028MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4153f9a9-3973-4041-aa6a-0e2e47ccd769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "!pip install transformers datasets accelerate peft huggingface_hub hf_transfer flash-attn trl wandb -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4345e0da-0b32-426c-b49b-94890a933ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /opt/app-root/src/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\n",
    "  token=\"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\", # ADD YOUR TOKEN HERE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32585b96-b68f-42d9-9d99-d3201055ddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from IPython.display import Markdown\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, EarlyStoppingCallback\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset \n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8966a1d-54cb-41ee-bf4d-bda33150d2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Bfloat16 avaiable?: True\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Is Bfloat16 avaiable?: {torch.cuda.is_bf16_supported()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ceeb49-b30b-4df8-bc18-cf4398a1daf7",
   "metadata": {},
   "source": [
    "### 1. Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2630aeb3-093f-44b4-a7ad-f7f71b939d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Deci/DeciLM-7B\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b988fa55-9822-400b-ab93-8b7714e76981",
   "metadata": {},
   "source": [
    "#### 1.1 Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d457500-d38d-449d-9487-caac26a897b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3233e4b7d00f4159b30eab488e6e44a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be250617-869f-43fb-a6a5-7c53b15b000e",
   "metadata": {},
   "source": [
    "#### 1.2 Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f884a61-4768-4bb2-b5e0-316f1f7e3bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    padding_side=\"left\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb2f44c8-66ec-4a25-a8f0-b25c7e40aa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size of DeciLM7B: 32,000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size of DeciLM7B: {len(tokenizer.get_vocab()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f2f21d0-0154-4928-ad8a-f5284fee129d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c066a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0a837a-cf38-48a9-9f85-40e8c0249051",
   "metadata": {},
   "source": [
    "#### 1.3 Inferece test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38232e6e-6f3c-4e53-bcce-8bd13f060a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 1,\n",
    "    \"top_k\": 100,\n",
    "    \"top_p\":0.90,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9c4cc8c-dd4a-4e18-b1d1-6a359875a579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Write me a poem about Machine Learning. [/INST]\n",
       "Answer: In the garden of technology, a seed was born\n",
       "That would grow and flower, a budding Machine Learning torn\n",
       "From petals of AI and logic trees\n",
       "To data flows, code and memories\n",
       "\n",
       "With algorithms and models, it spread its wings\n",
       "To process images, speech and all manner of things\n",
       "To analyze patterns, predict outcomes\n",
       "To make decisions, with no human frowns\n",
       "\n",
       "It learned to play chess and Go with ease"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(text=input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**input_ids, **generation_config)\n",
    "Markdown(tokenizer.decode(token_ids=outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b0ed60-bb49-48d1-8f3f-3b28729d1a63",
   "metadata": {},
   "source": [
    "### 2. Train data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d8d3d2-047e-47bc-9f5f-93d03cc46bd6",
   "metadata": {},
   "source": [
    "#### 2.1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07c8a3ba-1be3-4f32-a680-7bad0527528c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86272db-2765-4c5b-82a4-a4b29471f78c",
   "metadata": {},
   "source": [
    "#### 2.2 Split into test and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae92bcd3-095a-431c-9af6-f3808967b71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78477 100\n"
     ]
    }
   ],
   "source": [
    "train_test_split = dataset.train_test_split(test_size=100, seed=1399, shuffle=True)\n",
    "train_data = train_test_split[\"train\"].shuffle()\n",
    "val_data = train_test_split[\"test\"].shuffle()\n",
    "print(len(train_data), len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa447995-9979-4c9d-a94f-dd03bc494a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "sample = train_data[torch.randint(low=0, high=len(train_data), size=(1,)).item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7c6e24-7837-4566-b465-8727d828c097",
   "metadata": {},
   "source": [
    "#### 2.2 Testing baseline inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dce71678-15dc-4d4c-b1ac-cf1b04e5c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\\n\\n\" + \\\n",
    "\"You must output the SQL query that answers the question.\\n\\n\" + \\\n",
    "\"### Input:\\n\" + \\\n",
    "\"```{question}```\\n\\n\" + \\\n",
    "\"### Context:\\n\" + \\\n",
    "\"```{context}```\\n\\n\"\n",
    "# \"### Response:\\n\" + \\\n",
    "# \"```{response}```\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd6699d7-384f-46bc-bef2-519d226cf9f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```What country has a medical school established in 1969 with both an IMED and avicenna?```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_name_15 (country_territory VARCHAR, imed_avicenna_listed VARCHAR, established VARCHAR)```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(template.format(question=sample[\"question\"], context=sample[\"context\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b65c74f3-8aa8-4ca1-ae94-6d451f666289",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template.format(context=sample[\"context\"], question=sample[\"question\"])\n",
    "input_ids = tokenizer(text=prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**input_ids, **generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3e97399-b003-471f-85e8-41a164753e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```COPY table_name_15(country_territory,imed_avicenna_listed,established) FROM stdin CSV HEADER```\n",
       "\n",
       "```cambridge,United Kingdom,true,true,1969```\n",
       "\n",
       "```cambridge,United States,true,true,1951```\n",
       "\n",
       "```madrid,Spain,true,true,19"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Answer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "SELECT country_territory FROM table_name_15 WHERE imed_avicenna_listed = \"both\" AND established = 1969"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(Markdown(\"#### Completion:\"))\n",
    "display(Markdown(tokenizer.decode(token_ids=outputs[0], skip_special_tokens=True).replace(prompt, \"\")))\n",
    "display(Markdown(\"#### Answer:\"))\n",
    "Markdown(sample[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac0bea2-c9da-47c3-b3ea-4cafad52718e",
   "metadata": {},
   "source": [
    "#### 2.3 Creating template function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92050802-48a8-4b1c-9e9d-7010eb7fdcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    template = \"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\\n\\n\" + \\\n",
    "    \"You must output the SQL query that answers the question.\\n\\n\" + \\\n",
    "    \"### Input:\\n\" + \\\n",
    "    \"```{question}```\\n\\n\" + \\\n",
    "    \"### Context:\\n\" + \\\n",
    "    \"```{context}```\\n\\n\" + \\\n",
    "    \"### Response:\\n\" + \\\n",
    "    \"```{answer};```\"\n",
    "\n",
    "    text = template.format(context=example[\"context\"], question=example[\"question\"], answer=example[\"answer\"])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3831a9b6-b1d1-4535-aada-bde7bb3321a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```Name the opponent for game 5```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_name_25 (opponent VARCHAR, game VARCHAR)```\n",
       "\n",
       "### Response:\n",
       "```SELECT opponent FROM table_name_25 WHERE game = 5;```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(formatting_func(train_data[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c598fe-753d-468f-8791-b16d8fc62b18",
   "metadata": {},
   "source": [
    "### 3. Parameter Efficient Fine-Tuning (PEFT) - LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbfb5331-e91c-4d02-b337-19a01ee9de2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b64730-1a79-464f-86b8-0726c232424a",
   "metadata": {},
   "source": [
    "#### 3.1 Prepare LoRA Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f90ae01f-1ec9-4ff5-ba09-99ab28c3f915",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "if model.config.to_dict()[\"use_cache\"]:\n",
    "    model.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42d72839-f62e-4c61-8b7b-3475d2af6dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6788ca75-065a-41c0-a1a7-bd04998cb54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model=model, peft_config=peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fccf984-2137-4e2d-9efa-8de7cd588a3f",
   "metadata": {},
   "source": [
    "#### 3.2 Check trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb19e1d8-5d24-4070-b0c7-c1699422d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63bebb3c-c17b-4721-9836-d333378f58a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 41168896 || all params: 7084720128 || trainable%: 0.5810941752983809\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de297270-16f7-4470-9329-6d86c74fe956",
   "metadata": {},
   "source": [
    "### 4. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc009739-c94d-41e6-b628-b3d34f1c49c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "args_definition = dict(\n",
    "    output_dir=\"./deci7bit-lora-sql\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=3e-4,\n",
    "    max_steps=500,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm = 0.3,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=20,\n",
    "    save_steps=20,\n",
    "    logging_first_step=True,\n",
    "    seed=1399,\n",
    "    bf16=True,\n",
    "    report_to=\"wandb\",\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "args = TrainingArguments(**args_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38ce2980-bfa0-46ba-88b7-f74c6bbc901f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, packing. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/opt/app-root/lib64/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/app-root/lib64/python3.9/site-packages/trl/trainer/sft_trainer.py:192: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.9/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cddc2d631ce34602be716955e0b607df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62724e832de04b19a8520ca9d1f839bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/trl/trainer/sft_trainer.py:408: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/opt/app-root/lib64/python3.9/site-packages/trl/trainer/sft_trainer.py:432: UserWarning: You passed `packing=True` to the SFTTrainer/SFTConfig, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    args=args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=1024,\n",
    "    packing=True,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abf601b1-fb2c-47db-b172-83551e936d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mliuxiangwin\u001b[0m (\u001b[33mliuxiangwin-free\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/opt/app-root/src/7b-SQLMaster-FineTune/wandb/run-20240819_032525-bbnb017r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/liuxiangwin-free/huggingface/runs/bbnb017r' target=\"_blank\">./deci7bit-lora-sql</a></strong> to <a href='https://wandb.ai/liuxiangwin-free/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/liuxiangwin-free/huggingface' target=\"_blank\">https://wandb.ai/liuxiangwin-free/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/liuxiangwin-free/huggingface/runs/bbnb017r' target=\"_blank\">https://wandb.ai/liuxiangwin-free/huggingface/runs/bbnb017r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='220' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [220/500 2:26:54 < 3:08:41, 0.02 it/s, Epoch 0/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.776200</td>\n",
       "      <td>0.507999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.482900</td>\n",
       "      <td>0.427779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.428300</td>\n",
       "      <td>0.401915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.410200</td>\n",
       "      <td>0.390339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.385620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.391900</td>\n",
       "      <td>0.380029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.387600</td>\n",
       "      <td>0.377312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.384500</td>\n",
       "      <td>0.374146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.380100</td>\n",
       "      <td>0.366520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.373300</td>\n",
       "      <td>0.361285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.369700</td>\n",
       "      <td>0.361334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=220, training_loss=0.4361744365908883, metrics={'train_runtime': 8858.361, 'train_samples_per_second': 1.806, 'train_steps_per_second': 0.056, 'total_flos': 3.007714272529613e+17, 'train_loss': 0.4361744365908883, 'epoch': 0.621030345800988})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4680fac3-56d5-42ed-be10-ae3b755b094e",
   "metadata": {},
   "source": [
    "#### 4.1 Compare outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6da176e-562c-4f66-a792-e87495f3db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3699db5-f733-4aca-a404-b309bfe28ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template.format(context=sample[\"context\"], question=sample[\"question\"])\n",
    "input_ids = tokenizer(text=prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = fine_tuned_model.generate(**input_ids, **generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a2f4e3b-12a7-44bf-99c8-fef117fb48fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```What country has a medical school established in 1969 with both an IMED and avicenna?```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_name_15 (country_territory VARCHAR, imed_avicenna_listed VARCHAR, established VARCHAR)```\n",
       "\n",
       "### Response:\n",
       "```SELECT country_territory FROM table_name_15 WHERE imed_avicenna_listed = \"yes\" AND established = \"1969\";```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Answer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "SELECT country_territory FROM table_name_15 WHERE imed_avicenna_listed = \"both\" AND established = 1969"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(Markdown(\"#### Completion:\"))\n",
    "display(Markdown(tokenizer.decode(token_ids=outputs[0], skip_special_tokens=True)))\n",
    "display(Markdown(\"#### Answer:\"))\n",
    "Markdown(sample[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b994a4-35c2-42ff-98d3-bd8ccec79a90",
   "metadata": {},
   "source": [
    "#### 4.2 Performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44877767-2bef-4060-95fc-d2b0e54cc796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7462d937ff0042608efd95a7413fafa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "not_tuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "fine_tuned_model.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58ece5f5-a99a-49e9-96b7-50f0f82662d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(example, ft_model, og_model):\n",
    "    prompt = template.format(context=example[\"context\"], question=example[\"question\"])\n",
    "    input_ids = tokenizer(text=prompt, return_tensors=\"pt\").to(device)\n",
    "    ft_outputs = ft_model.generate(**input_ids, **generation_config)\n",
    "    og_outputs = og_model.generate(**input_ids, **generation_config)\n",
    "\n",
    "    display(Markdown(\"#### Prompt:\"))\n",
    "    display(Markdown(prompt))\n",
    "    display(Markdown(\"#### Original Completion:\"))\n",
    "    display(Markdown(tokenizer.decode(token_ids=og_outputs[0], skip_special_tokens=True) \\\n",
    "           .replace(prompt, \"\")))\n",
    "    display(Markdown(\"#### Fine-tuned Completion:\"))\n",
    "    display(Markdown(tokenizer.decode(token_ids=ft_outputs[0], skip_special_tokens=True) \\\n",
    "           .replace(prompt, \"\")))\n",
    "    display(Markdown(\"#### Expected Answer:\"))\n",
    "    display(Markdown(\"`{answer}`\".format(answer=example[\"answer\"])))\n",
    "    display(Markdown(\"-----------------------------\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8066b6e2-f1d3-492c-be51-338a0f56fb90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Prompt:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```What is the name of the Tournament on oct 23?```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_name_36 (tournament VARCHAR, date VARCHAR)```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Original Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```Tournament```: ``(tournament VARCHAR(100), date VARCHAR(100), PRIMARY KEY (tournament, date))```\n",
       "\n",
       "```date```: ``(date VARCHAR(100), PRIMARY KEY (tournament, date))```\n",
       "\n",
       "```date```: ``(date VARCHAR(100), PRIMARY KEY (tournament, date"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Fine-tuned Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Response:\n",
       "```SELECT tournament FROM table_name_36 WHERE date = \"oct 23\";```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Expected Answer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "`SELECT tournament FROM table_name_36 WHERE date = \"oct 23\"`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "-----------------------------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Prompt:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```List the name of rooms with king or queen bed.```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE Rooms (roomName VARCHAR, bedType VARCHAR)```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Original Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```INSERT INTO Rooms VALUES ('104', 'king')```\n",
       "\n",
       "```INSERT INTO Rooms VALUES ('111', 'queen')```\n",
       "```\n",
       "\n",
       "### Output:\n",
       "```SELECT roomName FROM Rooms WHERE bedType IN ('king', 'queen')``` "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Fine-tuned Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Response:\n",
       "```SELECT roomName FROM Rooms WHERE bedType = 'King' OR bedType = 'Queen';```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Expected Answer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "`SELECT roomName FROM Rooms WHERE bedType = \"King\" OR bedType = \"Queen\"`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "-----------------------------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Prompt:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```Which kickoff had an attendance of 58,120?```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_name_9 (kickoff_ VARCHAR, a_ VARCHAR, attendance VARCHAR)```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Original Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```INSERT INTO table_name_9 VALUES ('Sunday, January 11, 2009', 'Minnesota Vikings', '65,321')```\n",
       "\n",
       "```INSERT INTO table_name_9 VALUES ('Sunday, January 11, 2009', 'Dallas Cowboys', '65,321')```\n",
       "\n",
       "```INSERT INTO table_"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Fine-tuned Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Response:\n",
       "```SELECT kickoff_[a_] FROM table_name_9 WHERE attendance = \"58,120\";```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Expected Answer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "`SELECT kickoff_[a_] FROM table_name_9 WHERE attendance = \"58,120\"`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "-----------------------------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Prompt:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```Who is the March playmate with a November playmate Cara Wakelin?```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_name_3 (march VARCHAR, november VARCHAR)```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Original Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```march```\n",
       "```November```\n",
       "\n",
       "```SARA WAKELIN```\n",
       "```march playmate```\n",
       "```november playmate```\n",
       "\n",
       "### Output:\n",
       "```SELECT * FROM table_name_3 WHERE March='march playmate' AND November='november playmate'``` "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Fine-tuned Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Response:\n",
       "```SELECT march FROM table_name_3 WHERE november = \"caral wakelin\";```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Expected Answer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "`SELECT march FROM table_name_3 WHERE november = \"cara wakelin\"`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "-----------------------------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Prompt:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```What is every construction date for the registration of HB-HOS?```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_22180353_1 (construction VARCHAR, registration VARCHAR)```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Original Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "| construction | registration |\n",
       "|---|---|\n",
       "| 06.11.1945 | HB-HOS |\n",
       "| 10.01.1951 | HB-HOS |\n",
       "| 12.09.1953 | HB-HOS |\n",
       "| 26.01.1955 | HB-HOS |\n",
       "| 06.11"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Fine-tuned Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Response:\n",
       "```SELECT construction FROM table_22180353_1 WHERE registration = \"HB-HOS\";```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Expected Answer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "`SELECT construction AS date FROM table_22180353_1 WHERE registration = \"HB-HOS\"`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "-----------------------------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    generate_responses(val_data[i], ft_model=fine_tuned_model, og_model=not_tuned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ee167f-332f-47fe-814a-6f47328b08f7",
   "metadata": {},
   "source": [
    "### 5. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fcc16e89-67b3-4950-abef-59a1afe9c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_name = \"deci7b-ft-lora-sql-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e682914e-1dc3-4c1a-a6b5-e18b4f40cc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "906c475dad404c65a8a232416d4a1988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6dfadb7bbc4fb197795dfda447d593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5cfcf77d5e4acd80f96615f745d77b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b30ccf66ef4b83a46aebab844cc807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b42f68c302d4d66a16fe07f4a91c054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Liu-Xiang/deci7b-ft-lora-sql-v2/commit/cdfa7a8de1260bd190e6604cea385deeac53e5e2', commit_message='Upload tokenizer', commit_description='', oid='cdfa7a8de1260bd190e6604cea385deeac53e5e2', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model & tokenizer\n",
    "fine_tuned_model.push_to_hub(model_save_name)\n",
    "tokenizer.push_to_hub(model_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2488a39e-3a93-465e-8455-50effbb22267",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad09eb90c53a44d0aab0e6461b74aa06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec4db0b6842a467e8268ece7effef052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/40.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c75cf53efa54e4b99db030fd1ec8c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Liu-Xiang/deci7bit-lora-sql/commit/f42398f42c107f48a1cfeb9f7d1a5475fffda021', commit_message='deci7b-ft-lora-sql-v2adapters', commit_description='', oid='f42398f42c107f48a1cfeb9f7d1a5475fffda021', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save adapters\n",
    "trainer.push_to_hub(model_save_name + \"adapters\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
